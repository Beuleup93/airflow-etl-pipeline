# ğŸš€ Projet 2 - Orchestration ETL avec Apache Airflow

Ce projet implÃ©mente l'orchestration du pipeline ETL e-commerce en utilisant Apache Airflow avec la syntaxe moderne.

## ğŸ¯ Objectifs

- **Orchestration** : Automatiser l'exÃ©cution du pipeline ETL
- **Monitoring** : Surveiller les performances et les erreurs
- **Scheduling** : Planifier l'exÃ©cution des tÃ¢ches
- **Alerting** : Notifications en cas d'Ã©chec
- **Data Quality** : ContrÃ´les de qualitÃ© automatisÃ©s

## ğŸ—ï¸ Architecture

```
airflow-etl/
â”œâ”€â”€ dags/                          # DAGs Airflow
â”‚   â”œâ”€â”€ etl_ecommerce_dag.py      # DAG principal ETL
â”‚   â”œâ”€â”€ data_quality_dag.py        # DAG contrÃ´les qualitÃ©
â”‚   â”œâ”€â”€ monitoring_dag.py          # DAG monitoring
â”‚   â””â”€â”€ exampledag.py              # DAG d'exemple Astro
â”œâ”€â”€ airflow_settings.yaml          # Configuration Airflow
â”œâ”€â”€ requirements.txt               # DÃ©pendances Python
â””â”€â”€ README.md                     # Documentation
```

## ğŸ› ï¸ Technologies

- **Apache Airflow 2.7+** : Orchestrateur de workflows
- **Docker & Docker Compose** : Environnement de dÃ©veloppement
- **PostgreSQL** : Base de donnÃ©es
- **Python** : Langage de programmation
- **Astro CLI** : Outil de gestion Airflow moderne

## ğŸš€ Installation et DÃ©marrage

### PrÃ©requis
- Docker Desktop
- Astro CLI installÃ©

### Ã‰tapes

1. **DÃ©marrer Airflow**
   ```bash
   cd airflow-etl
   astro dev start
   ```

2. **AccÃ©der Ã  l'interface Airflow**
   - URL : http://localhost:8080
   - Login : admin
   - Password : admin

3. **Activer les DAGs**
   - Aller dans l'interface Airflow
   - Activer `etl_ecommerce_pipeline`
   - Activer `data_quality_checks`
   - Activer `monitoring_dag`

## ğŸ“Š DAGs Disponibles

### 1. ETL E-commerce Pipeline (`etl_ecommerce_pipeline`)
- **FrÃ©quence** : Quotidienne
- **TÃ¢ches** :
  - `extract_products` : Extraction API Fake Store
  - `extract_orders` : Extraction CSV (simulation)
  - `extract_sessions` : Extraction JSON (simulation)
  - `transform_data` : Transformation et nettoyage
  - `load_data` : Chargement PostgreSQL
  - `send_notification` : Notification de fin
- **DÃ©pendances** : Extraction en parallÃ¨le â†’ Transformation â†’ Chargement â†’ Notification

### 2. Data Quality Checks (`data_quality_checks`)
- **FrÃ©quence** : Quotidienne
- **TÃ¢ches** :
  - `check_data_completeness` : VÃ©rification complÃ©tude
  - `check_data_consistency` : VÃ©rification cohÃ©rence
  - `check_data_quality` : VÃ©rification qualitÃ©
  - `generate_quality_report` : Rapport de qualitÃ©
- **DÃ©pendances** : VÃ©rifications en parallÃ¨le â†’ Rapport

### 3. Monitoring (`monitoring_dag`)
- **FrÃ©quence** : Toutes les heures
- **TÃ¢ches** :
  - `check_dag_status` : Statut des DAGs
  - `check_database_health` : SantÃ© de la DB
  - `check_system_resources` : Ressources systÃ¨me
  - `send_health_report` : Rapport de santÃ©
- **DÃ©pendances** : VÃ©rifications en parallÃ¨le â†’ Rapport

## ğŸ”§ FonctionnalitÃ©s

### Syntaxe Moderne Airflow 2.0+
- **`@dag`** : DÃ©corateur pour dÃ©finir les DAGs
- **`@task`** : DÃ©corateur pour dÃ©finir les tÃ¢ches
- **DÃ©pendances automatiques** : GÃ©rÃ©es par les paramÃ¨tres des fonctions
- **XCom automatique** : Partage de donnÃ©es entre tÃ¢ches

### Monitoring et Alerting
- **VÃ©rifications de santÃ©** : Base de donnÃ©es, systÃ¨me, DAGs
- **Rapports automatiques** : QualitÃ© des donnÃ©es, santÃ© du systÃ¨me
- **Notifications** : Email en cas d'Ã©chec

### QualitÃ© des DonnÃ©es
- **ComplÃ©tude** : VÃ©rification des donnÃ©es manquantes
- **CohÃ©rence** : VÃ©rification des relations entre tables
- **QualitÃ©** : Validation des formats et valeurs

## ğŸ“š Concepts Appris

- **DAGs** : Directed Acyclic Graphs
- **Tasks** : UnitÃ©s de travail atomiques
- **Scheduling** : Planification des exÃ©cutions
- **XCom** : Partage de donnÃ©es entre tÃ¢ches
- **Monitoring** : Surveillance des performances
- **Data Quality** : ContrÃ´les automatisÃ©s
- **Syntaxe moderne** : `@dag` et `@task` decorators

## ğŸ§ª Tests

```bash
# Tests des DAGs
python scripts/validate_dags.py

# Tests unitaires
pytest tests/
```

## ğŸ“ Prochaines Ã‰tapes

- **Projet 3** : AWS Cloud (S3, Glue, EMR)
- **Projet 4** : Databricks et Spark
- **Projet 5** : dbt et Snowflake
- **Projet 6** : Data Governance
- **Projet 7** : DevOps et DataOps
- **Projet 8** : Monitoring avec Grafana
- **Projet 9** : Projet final intÃ©grÃ©

## ğŸ¤ Support

Pour toute question ou problÃ¨me, consultez la documentation Airflow ou ouvrez une issue.