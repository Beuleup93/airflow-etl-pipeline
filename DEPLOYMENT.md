# Guide de D√©ploiement - Airflow ETL Pipeline

Ce guide explique comment d√©ployer le projet Airflow ETL en utilisant Astro CLI pour le d√©veloppement et Docker Compose pour la production.

## D√©ploiement

### 1. D√©veloppement avec Astro CLI

Le d√©veloppement se fait avec Astro CLI qui fournit automatiquement PostgreSQL et Airflow :

#### D√©marrage
```bash
astro dev start
```

#### Services fournis automatiquement
- PostgreSQL (service `postgres`)
- Airflow Scheduler
- Airflow API Server (port 8080)
- Airflow DAG Processor
- Airflow Triggerer
### 2. Production avec Docker Compose

Pour la production, utilisez Docker Compose avec les fichiers fournis :

#### D√©marrage production
```bash
docker-compose -f docker-compose.prod.yml up -d
```

#### Services de production
- PostgreSQL avec pgAdmin
- Airflow Webserver
- Airflow Scheduler
- Volumes persistants pour les donn√©es

Avant le premier d√©ploiement, configurez les secrets GitHub :

```bash
# Secrets obligatoires
DB_HOST=your-db-host
DB_PORT=5432
DB_NAME=ecommerce_db
DB_USER=postgres
DB_PASSWORD=your-secure-password

# Secrets optionnels
SLACK_WEBHOOK=your-slack-webhook-url
DOCKER_REGISTRY=your-registry
DOCKER_USERNAME=your-username
DOCKER_PASSWORD=your-password
```

## üê≥ **D√©ploiement Local**

### **1. D√©ploiement en d√©veloppement**

```bash
# Cloner le repository
git clone <your-repo-url>
cd airflow-etl

# D√©marrer les services
docker-compose up -d

# V√©rifier le statut
docker-compose ps
```

### **2. D√©ploiement en production**

```bash
# Utiliser le script de d√©ploiement
./scripts/deploy.sh production

# Ou utiliser docker-compose directement
docker-compose -f docker-compose.prod.yml up -d
```

## üîß **Configuration des Environnements**

### **1. Variables d'environnement**

#### **D√©veloppement**
```bash
# .env.development
DB_HOST=localhost
DB_PORT=5432
DB_NAME=ecommerce_db
DB_USER=postgres
DB_PASSWORD=postgres
AIRFLOW_ENV=development
```

#### **Production**
```bash
# .env.production
DB_HOST=prod-db-host
DB_PORT=5432
DB_NAME=ecommerce_db
DB_USER=prod_user
DB_PASSWORD=secure_prod_password
AIRFLOW_ENV=production
```

### **2. Configuration Docker**

#### **Dockerfile**
- Image de base : `apache/airflow:2.8.1-python3.11`
- D√©pendances : Install√©es via `requirements.txt`
- Configuration : Copi√©e depuis le repository
- Ports : 8080 (Airflow), 5432 (PostgreSQL)

#### **Docker Compose**
- **Services** : PostgreSQL, Airflow Webserver, Airflow Scheduler
- **Volumes** : DAGs, logs, donn√©es
- **R√©seau** : Communication inter-services
- **Sant√©** : Health checks automatiques

## üìä **Monitoring et Observabilit√©**

### **1. Endpoints de sant√©**

#### **Airflow**
- **UI** : http://localhost:8080
- **Health** : http://localhost:8080/health
- **API** : http://localhost:8080/api/v1/health

#### **PostgreSQL**
- **Port** : 5432
- **Health** : `pg_isready -U postgres`

#### **pgAdmin**
- **UI** : http://localhost:5050
- **Admin** : admin@example.com / admin

### **2. Logs et Monitoring**

#### **Logs Airflow**
```bash
# Voir les logs
docker-compose logs airflow-webserver
docker-compose logs airflow-scheduler

# Logs en temps r√©el 
docker-compose logs -f airflow-webserver
```

#### **Logs PostgreSQL**
```bash
# Voir les logs
docker-compose logs postgres
```

## üîÑ **Rollback et R√©cup√©ration**

### **1. Rollback automatique**

```bash
# Via GitHub Actions
# 1. Allez sur l'onglet "Actions"
# 2. S√©lectionnez "Rollback Deployment"
# 3. Cliquez sur "Run workflow"
# 4. Choisissez l'environnement et la version
```

### **2. Rollback manuel**

```bash
# Arr√™ter les services
docker-compose down

# R√©cup√©rer une version pr√©c√©dente
git checkout <previous-commit>

# Red√©marrer avec la version pr√©c√©dente
docker-compose up -d
```

## üö® **D√©pannage**

### **1. Probl√®mes courants**

#### **Service non accessible**
```bash
# V√©rifier le statut
docker-compose ps

# Red√©marrer un service
docker-compose restart airflow-webserver
```

#### **Base de donn√©es non accessible**
```bash
# V√©rifier la connexion
docker-compose exec postgres pg_isready -U postgres

# Voir les logs
docker-compose logs postgres
```

#### **DAGs non charg√©s**
```bash
# V√©rifier les DAGs
docker-compose exec airflow-webserver airflow dags list

# Forcer le rechargement
docker-compose restart airflow-webserver
```

### **2. Logs de d√©bogage**

```bash
# Logs d√©taill√©s
docker-compose logs --tail=100 airflow-webserver

# Logs en temps r√©el
docker-compose logs -f airflow-scheduler
```

## üìà **Optimisations**

### **1. Performance**

#### **Ressources**
- **CPU** : Minimum 2 cores
- **RAM** : Minimum 4GB
- **Stockage** : Minimum 20GB

#### **Configuration**
```yaml
# docker-compose.prod.yml
services:
  airflow-webserver:
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G
```

### **2. S√©curit√©**

#### **Secrets**
- Utilisez des secrets GitHub
- Chiffrez les donn√©es sensibles
- Rotation r√©guli√®re des mots de passe

#### **R√©seau**
- Limitez l'acc√®s aux ports
- Utilisez des r√©seaux priv√©s
- Configurez des firewalls

## üìû **Support**

### **1. Documentation**
- [Airflow Documentation](https://airflow.apache.org/docs/)
- [Docker Compose](https://docs.docker.com/compose/)
- [GitHub Actions](https://docs.github.com/en/actions)

### **2. Contact**
- **Issues** : Cr√©ez une issue sur GitHub
- **Discussions** : Utilisez les discussions GitHub
- **Email** : beuleup2018@gmail.com

---

**üéâ F√©licitations ! Votre pipeline Airflow ETL est maintenant pr√™t pour le d√©ploiement automatique !**
